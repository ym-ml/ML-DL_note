## 1.

Import torch

torch语法类似numpy

torch.tensor\==np.array


## 2.转换:

torch(X)-->np :arr=X.numpy()

np-->torch  torch.tensor(arr)

## 3.

X.clone()  ==  arr.copy()
torch  `dim` `axis`
numpy `axis`


## 4.

|                         |                    |
| ----------------------- | ------------------ |
| X.shape                 | >>>(2,3,5)         |
| X.sum(axis=(1,2)).shape | >>>torch.Size([2]) |


## 5.

numpy and torch have  "keepdims=True" to keep the axis when use sum,mean…

eg:(2,3,5)   sum(axis=0)-->(3,5)

   sum(axis=0,keepdims=True)-->(1,3,5)

it's used to broadcast

## 6.一个向量x,通常默认是列向量

点积:torch.dot两个向量相同位置元素乘积的和(又叫内积)XTX-->数

外积:XXT-->矩阵

矩阵-向量积:torch.mv(A,x) A的列维数等于x的长度

矩阵-矩阵乘法:A的每一行和B的每一列相乘  torch.mm(A,B)

np: @/matmul

torch: @/mm

~~np:dot 对二维数组也可,一维数组和@/matmul会不同;  torch:dot 只能对一维(知道即可)~~

范数(norm):刻画矩阵\向量的大小

torch.norm(X):L2范数(欧氏距离)

L1范数:绝对值的和

## 7.自动求导

首先创建自变量x的时候需要x=torch.arange(4.0,requirs_grad=True)或者

x=… then

x.requirs_grad_(True)  (应该是对象的属性)

y.backward()得到梯度之后需要清理:x.grad=None or x.grad.zero_()

|      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ---- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 对标量: | X=[0,1,2,3]<br><br>y=torch.dot(x,x)<br><br>y.backward()<br><br>即可得到梯度:x.grad        x.grad\==4*x (4*X是求导)                                                                                                                                                                                                                                                                                                                                                                                          |
| 对向量: | y.backward()需要一个gradient参数,gradient是一个和y 长度相同的向量,并且实际上是<br><br>1.将向量 y 与传入的 gradient 参数(权重)进行点积，得到一个标量<br><br>2.然后对这个标量求梯度<br><br># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。  <br># 本例只想求偏导数的和，所以传递一个1的梯度是合适的  <br>x.grad.zero_()  <br>y = x * x  <br># 等价于y.backward(torch.ones(len(x)))  <br>y.sum().backward()  <br>x.grad<br><br>tensor([0., 2., 4., 6.])<br><br>来自 <[https://zh.d2l.ai/chapter_preliminaries/autograd.html](https://zh.d2l.ai/chapter_preliminaries/autograd.html)> |

  在进行了一次自动求导z.backward()之后,torch默认**清空计算图**中的中间变量(梯度信息\中间结果),所以然后再次调用z.backward()会报错.可以用`z.backward(return_garph=True)`保存
  

## 8.分离计算 ##
  x-->y-->z,希望y是当作一个**常数**,y=x\*x   z=y\*x ,z对x求导,希望是y而不是3\*x^2
  ```python
  x.grad.zero_()
  y=x*x
  
  u=y.detach()
  z=u*x
  
  z.sum().backward()
  x.grad==u
  >>>True
   
  ```
  use   **.detach**
 Meanwhile we can get dy/dx:
 ```python
 x.grad.zero_()
 y.backward()
 x.grad==2*x	 
 ```
### tips:###
 **1.**
	torch.norm()
	torch.linalg.matrix_norm()
	torch.linalg.vector_norm()
	torch.linalg.norm()
	
		ord                    
			'fro'  Frobenius  (p=2)
			'nuc'  nuclear norm
			Number                 standard normal
		p;dim
**2.**
```python
torch.randn(tuple of ints size,*,torch.Generator/generator,dtype=None,requires_grad=bool)
```
```python
torch.randn(1)
>>>tensor([2.4061])
torch.randn(size=(1,))
>>>tensor([1.3241])
```

## 9.python 控制流的梯度计算

# 2.6 概率


P99

