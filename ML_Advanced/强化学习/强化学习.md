# 1.任务与奖赏
我们考虑如何种西瓜,播种,浇水,施肥...最后种出一个瓜
若将得到好瓜作为最终奖赏, 在过程中每一步进行后,我们并不能立即获得这个奖赏,仅能得到一个反馈(eg:瓜苗看起来更健壮)
我们需多次种瓜,不断摸索,才能总结出经验.此即 **强化学习(reinforcement learning)**

![[Pasted image 20251231153411.png]]
强化学习任务通常用马尔可夫决策过程(Markov Decision Process,MDP)来描述:
1. 机器处于环境 $E$ 中
2. 状态空间为 $X$ ,其中每个状态$x \in X$ 是机器感知到的环境的描述.
3. 机器能采取的动作构成动作空间 $A$.
4. 若某个动作 $a \in A$ 作用在状态 $x$上,则潜在的转移函数 $P$ 将使环境从当前状态按某一概率转移到另一状态.
5. 在转移到另一状态的同时,环境会根据潜在的"奖赏"(reward)函数 $R$反馈给机器一个奖赏.
6. 综合起来,强化学习任务对应了四元组 $E=<X,A,P,R>$.

一个给西瓜浇水的简单例子:![[Pasted image 20251231154241.png]]

需注意机器与环境的界限. 
*eg:种西瓜中,环境是西瓜生长的自然世界;下棋中,环境是棋盘和对手.*

环境中状态的转移/奖赏的返回是不受机器控制的,
机器只能通过选择要执行的动作来影响环境,也只能通过观察转移后的状态和返回的奖赏来感知环境.

机器要做的是通过在环境中不断尝试而学得一个策略(policy) $\pi$ ,根据这个策略,在状态 $x$ 下就能得知要执行的动作 $a=\pi (x)$ .
策略有两种表示方法:
1. 函数 $\pi : X \mapsto A$,确定性策略
2. 概率 $\pi : X \times A \mapsto \mathbb R$,随机性策略 $\pi (x,a)$为状态 $x$下选择动作$a$ 的概率,这里必须有**概率和为1**.

策略的优劣取决于长期执行这一策略的累积奖赏.学习的目的就是找到使长期累积奖赏最大化的策略.
长期累计奖赏常用的算法:
1. T步累积奖赏: $\mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T}r_t\right]$
2. $γ$ 折累积奖赏: $\mathbb{E}\left[\sum_{t=0}^{+\infty}\gamma^t r_{t+1}\right]$ 
其中,$r_t$表示第 t 步获得的奖赏值.

----
对比:

| 强化学习      | 监督学习    |
| --------- | ------- |
| 状态        | 示例      |
| 动作(连续/离散) | 标记      |
| 策略        | 分类器/回归器 |
不同: 强化学习中没有监督学习中的有标记样本(即"示例-标记"对)
*换言之，没有人直接告诉机器在什么状态下应该做什么动作，只有等到最终结果揭晓，才能通过 "反思"之前的动作是否正确来进行学习*

强化学习在某种意义上可看作具有"延迟标记信息"的监督学习问题.

# 2. K-摇臂赌博机
## 2.1 探索与利用

我们不妨先仅考虑最大化单步奖赏.
*注意,即使只考虑单步,与监督学习仍有和大不同,没有数据告诉机器应当做哪个动作*

欲最大化单步奖赏,考虑两个方面:
1. 知道每个动作带来的奖赏
2. 执行奖赏最大的动作
简单的想法,每个动作都试一下就知道了.
问题在于:一般一个动作的奖赏值是来自于一个概率分布,仅通过一次尝试无法确切获得奖赏期望

单步强化学习任务对应了一个理论模型,"K-摇臂赌博机"(K-armed bandit)
	K-摇臂赌博机有 K 个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币. 但这个概率赌徒并不知道.赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币.
	![[Pasted image 20251231164507.png]]

- 仅探索法:仅为获知每个摇臂的期望奖赏.
	轮流按下各个摇臂,最终将吐出硬币的平均概率作为奖赏期望的近似
- 仅利用法: 仅为执行奖赏最大的动作
	按下目前最优的摇臂.若多个同样,随机选一个
显然,总尝试次数有限, "探索"(估计摇臂的优劣)和"利用"(选择当前最优摇臂)这两者是矛盾的.  
**"探索-利用窘境"**(Exploration-Exploitation dilemma)
显然，欲累积奖赏最大，则必须在探索与利用之间达成较好的折中.

## 2.2 $\epsilon$-贪心

>[!tldr]
>$\epsilon$-贪心法基于一个概率来对探索和利用进行折中:
>每次尝试时,以 $\epsilon$ 的概率进行探索,以 $1-\epsilon$ 的概率进行利用

令 $Q(k)$ 记录摇臂 $k$ 的平均奖赏。若摇臂 $k$ 被尝试了 $n$ 次，得到的奖赏为 $v_1, v_2, \ldots, v_n$，则平均奖赏为: $$Q(k) = \frac{1}{n} \sum_{i=1}^{n} v_i .$$
这个式子不便于计算,我们改写成可增量计算的递推公式:$$
\begin{aligned}
Q_n(k) &= \frac{1}{n} ((n-1) \times Q_{n-1}(k) + v_n)\\
&=Q_{n-1}(k)+ \frac{1}{n}(v_n-Q_{n-1}(k))
\end{aligned}\tag{16.3}$$
算法如图:![[Pasted image 20251231170359.png]]

若当前摇臂奖赏的不确定性较大,(eg:概率分布较宽),则需更多探索,设置较大的$\epsilon$ 值;否则,需要较小的$\epsilon$
当尝试次数很多时,不在需要探索,这种情况可令$\epsilon$ 的值随尝试次数的增加而减少.(eg:令$\epsilon =\frac{1}{\sqrt t}$)

## 2.3 Softmax
>[!tldr]
>基于奖赏的高低为每个摇臂设置一个选取概率
>奖赏高的摇臂选中的概率高

Softmax中摇臂概率的分配是基于Boltzmann分布:$$P(k) = \frac{e^{\frac{Q(k)}{\tau}}}{\sum_{i=1}^{K} e^{\frac{Q(i)}{\tau}}} \tag{16.4}$$
其中, $Q(i)$ 记录当前摇臂的平均奖赏; $\tau>0$称为"温度",$\tau$ 越小,则平均奖励高的摇臂被选中的概率越高.
$\tau$ 趋于0时Softmax将趋于"仅利用", 趋于无穷大时Softmax则将趋于"仅探索"

算法如图![[Pasted image 20251231171924.png]]

## 2.4 补充
1. $\epsilon$-贪心算法与 Softmax算法孰优孰劣，主要取决于具体应用.
2. 对于离散状态空间、离散动作空间上的**多步**强化学习任务，一种直接的办法是将每个状态上动作的选择看作一个K-摇臂赌博机问题，用强化学习任务的**累积奖赏**来**代替**K-摇臂赌博机算法中的奖赏函数.<br>    当然,这种做法没有考虑马尔可夫决策过程,有很多局限

# 3.有模型学习
