# 1.基础知识
**computational learning theory**:研究机器学习的理论基础
## 一些定义
本章主要讨论二分类问题,若无特别说明,$y_i \in \mathcal{Y} = \{-1, +1\}$
给定样例集$D = \{ (x_1, y_1), (x_2, y_2), \ldots, (x_m, y_m) \},x_i \in \mathcal{X}$,假设$\mathcal{X}$中样本都是从$\mathcal{D}$中独立同分布采用而得

令$h$为从$\mathcal{X}到\mathcal{Y}$的一个映射(==学习器学的就是这个映射h)==,其泛化误差: $$E(h; D) = P_{x \sim D}(h(x) \neq y)$$
$h$在$D$上经验误差:$$\hat{E}(h; D) = \frac{1}{m} \sum_{i=1}^{m} I(h(x_i) \neq y_i)$$
由独立同分布采样,经验误差的期望应该等于泛化误差.
简记:$E(h; D)\longrightarrow E(h)$    $\hat{E}(h; D)\longrightarrow \hat{E}(h)$
令$E(h) \le \epsilon$, $\epsilon$表示预先设定的学得模型应满足的误差要求,"误差参数"

==后文将研究经验误差与泛化误差之间的逼近程度==
若$h$在数据集上的经验误差为0,即$\hat{E}(h)=0$则称$h$与$D$一致,否则称其与D不一致
对任意两个映射$h_{1},h_{2}\in \mathcal{X}\rightarrow\mathcal{Y}$,可通过其"不合"来度量他们之间的差别$$
d(h_1, h_2) = P_{x \sim D}(h_1(x) \neq h_2(x))$$

即二者预测结果不同的概率

##  一些不等式
>![[Pasted image 20251220115914.png]]

# 2.PAC学习
概率近似正确(Probably Approximately Correct,PAC)
## 一些标记
c: "概念",$\mathcal{X}\rightarrow\mathcal{Y}$的映射.    若对任何样例$(x,y)$有$c(x)=y$成立,则称c为目标概念;
$\mathcal{C}$ :所有希望学得的目标概念的集合
==我们学习的目标就是找到这个映射c==

$\mathcal{L}$: 学习算法
$\mathcal{H}$:假设空间.  这个学习算法能考虑到的所有可能概念的集合 (比如用决策树,SVM,那就有不同的假设空间$\mathcal{H_1},\mathcal{H_2}$)
==显然通常没有这么巧,$\mathcal{H}$与$\mathcal{C}$不同==
对$h \in \mathcal{H}$,不能确定$h$是不是真的概念,所以称$h$为假设

若目标概念$c \in\mathcal{H}$,则$\mathcal{H}$中存在假设能将所有样例正确分类,那么称该问题对学习算法$\mathcal{L}$是"可分的",亦称"一致的".若$c \notin\mathcal{H}$,则称"不可分的"

## 定义
我们希望学的假设h尽可能接近概念c.
(*由于现实约束,会存在在训练集上等效的假设h,此时无法区分;存在各种偶然性...所以是尽可能接近*)
也就是以较大的概率,学的误差满足预设上限的模型,即=="概率""近似正确"==
>![[Pasted image 20251220122428.png]]
>==泛化误差小于上限的概率大于一个预设值==
>$\delta$ 表示置信度,这样的学习算法$\mathcal{L}$能以较大的概率(至少$1-\delta$)学得目标概念c的近似(误差最多为$\epsilon$)

在此基础上,定义:
>![[Pasted image 20251220122917.png]]
>==样例数量满足一定条件PAC理论才认为可学习==

考虑时间问题,
>![[Pasted image 20251220123234.png]]
>==能在一定时间内解决才是高效==

假定学习算法$\mathcal{L}$处理每个样本的时间为常数,则时间复杂度等价于样本复杂度
>![[Pasted image 20251220123520.png]]

## PCA总结
**PAC学习给出了一个抽象地刻画机器学习能力的框架**

有一个关键因素:假设空间$\mathcal{H}$的复杂度.一般而言,$\mathcal{H}$越大,其包含任意目标概念的可能性越大,但是从中找出具体某个目标概念的难度也越大.$|\mathcal{H}|$有限时,称$\mathcal{H}$为"有限假设空间",无限时,为"无限假设空间"

下面分情况讨论

# 3.有限假设空间
## 3.1可分情形
可分,意味着$c \in \mathcal{H}$
给定包含m个样本的训练集D,如何找出满足误差参数的假设?

一种简单的想法,对每一个假设h,依次尝试,在训练集上出现了错误,则舍去.在训练集足够大的前提下,最终剩下的一个就是c. 当然,通常情况下,假设空间$\mathcal{H}$中可能存在多个等效的h.

到底需要多少个样例才能学得目标概念c的有效近似呢?
对PCA来说,只要训练集的规模能使学习算法$\mathcal{L}$以概率$1-\delta$找到目标假设的$\epsilon$近似即可

先找到在训练集上表现完美,但是泛化误差大于$\epsilon$的假设出现的概率:
	泛化误差大于$\epsilon$的概率![[Pasted image 20251220134324.png]]
	由独立同分布,h与D表现一致的概率:![[Pasted image 20251220134457.png]]
然后,令这种不好的假设出现的概率之和不大于$\delta$,解出m
	![[Pasted image 20251220135024.png]]

由此可知，有限假设空间$\mathcal{H}$都是PAC可学习的，所需的样例数目如式(12.14)所示，输出假设 h 的泛化误差随样例数目的增多而收敛到0，收敛速率为$O(\frac{1}{m})$.
## 3.2不可分情形
目标概念c不在假设空间$\mathcal{H}$中,对于任意$h\in \mathcal{H},\hat{E}(h) \not= 0$.
>![[Pasted image 20251220140018.png]]
>$\epsilon =\sqrt \frac{\ln|\mathcal{H}|+\ln(2/\delta)}{2m}$

可以证明给定假设空间$\mathcal{H}$,其中必存在一个泛化误差最小的假设.找出此假设的$\epsilon$近似也不错
so,可以将PAC学习推广到$c \notin \mathcal{H}$的情况:
>![[Pasted image 20251220140809.png]]

# 4.VC维
