# 10.1 k邻近学习(KNN)
## 1.过程
给定测试数据,在训练集中寻找与它**最近**的k个'邻居',根据这k个样本进行预测.(分类:投票,回归:平均,可以基于距离远近加权)
	**不同:** '懒惰学习'(lazy learning),训练只是把数据存起来,时间开销为0.
## 2.核心:
**距离**和**k
## 3.性能:

以k=1为例,x:测试样本 z最近邻样本
![[Pasted image 20251213093452.png]]
![[Pasted image 20251213094227.png]]

==就是贝叶斯最优分类器会把这个x分到c^\*==


![[Pasted image 20251213093632.png]]
x,z很近,所以P(c|x)=\=P(c|y) 
**k邻近的泛化错误率小于贝叶斯最优分类器错误率的2倍

## 10.2 低维嵌入

### 1.why?
前面的讨论基于测试样本总能在任意近的范围内找到z,但是当维度很高的时候,会出现样本稀疏,距离计算困难的问题,也是一个ML共同的问题----维数灾难(curse of dimensionality)
### 2.缓解方法
 1)降维/维数约简 :有些维度与结果关系并不密切
 2)特征选择(下一章)
**2.1'多维缩放'(MDS)**
	在d'维中的欧氏距离等于原始空间中的距离
**2.2 线性降维**
	![[Pasted image 20251213105440.png]]
	<span style="color:red">这里一列是一个样本</span>,将**X**从==d\*m==变成==d'\*m==
	简单来说就是把不需要的维度通过w设置成0来抹去<br>
	通常令d'<\<d
**2.3:评估**
	1.在测试集上评估
	2.对3d--2d,可以利用可视化,直观判断
## 10.3主成分分析(PCA)

(Principal Component Analysis),属于**线性降维** Z=W<sup>T</sup>X

==找一个超平面,来表达所有样本.这个超平面应该具有性质:
1.样本点到这个超平面的距离都足够近(**最近重构性**)
2.样本点在这个超平面上的投影尽可能分开(**最大可分性**)== (都挤在一起很难分类)
这两种推导等价

### 1)基于最大可分性:

#### 1.推导
最大化Z每一行的方差<span style="color:red">(Z的每一列代表一个样本)</span>

![[Pasted image 20251213135016.png]]
*Z每一行的方差 求和*
*均值z bar是0(已经**normalised**),忽略      **\*\*** 或者可以只中心化,就是减去平均值*

*观察可得,最后的结果实际上是ZZ<sup>T</sup>的对角线,即tr(ZZ<sup>T</sup>)  (**tr:矩阵的迹**,在后面求导的时候很有用)*

*因为结果可能有很多,所以这里限制了=**I**,即正交*

拉格朗日乘子法:![[Pasted image 20251213140103.png]]
*W:d\*d'*
*则 W<sup>T</sup>W=d'\*d'*

求导:![[Pasted image 20251213140440.png]]
*拆出来的这d'个式子符合特征值特征向量的定义*

![[Pasted image 20251213141036.png]]
*即**只需要对协方差矩阵XX<sup>T</sup>进行特征值分解,将求得的特征值从大到小排序,选前d'个即可***
*实践中,常通过对X进行**奇异值分解**来代替协方差矩阵的特征值分解*


#### 2.选择d'
1.用户事先指定
2.对kNN(或其他开销小的)在不同d'之下做交叉验证
3.对PCA,可以设置一个重构阈值![[Pasted image 20251213141941.png]]
**注:** 舍弃d-d',一方面降维,另一方面,可以去噪声

